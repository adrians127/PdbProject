{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PDB to similarity pipeline (CA-only)\n",
        "\n",
        "This notebook builds a CA-only distance pipeline from PDB files in `data/`,\n",
        "pads or trims to a fixed length, filters invalid samples, converts to\n",
        "similarity matrices, and prepares a PyTorch Dataset + DataLoader split.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    from torch.utils.data import Dataset, DataLoader, Subset\n",
        "except ImportError as exc:\n",
        "    raise ImportError(\n",
        "        \"This notebook requires torch. Install it before running the pipeline.\"\n",
        "    ) from exc\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "PDB_GLOB = \"*.pdb\"\n",
        "\n",
        "TARGET_L = 100\n",
        "CHAIN_STRATEGY = \"longest\"  # \"longest\" | \"single\" | \"concat\"\n",
        "SIGMA = 10.0\n",
        "PAD_VALUE = np.inf\n",
        "MIN_RESIDUES = 2\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "MAX_FILES = None  # set an int for a quick smoke run\n",
        "\n",
        "OUTPUT_DIR = Path(\"data\") / f\"precomputed_L{TARGET_L}\"\n",
        "MANIFEST_PATH = OUTPUT_DIR / \"manifest.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pdb_files = sorted(DATA_DIR.glob(PDB_GLOB))\n",
        "if MAX_FILES:\n",
        "    pdb_files = pdb_files[:MAX_FILES]\n",
        "print(f\"Found {len(pdb_files)} PDB files\")\n",
        "pdb_files[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def parse_ca_coords(pdb_path: Path, chain_strategy: str = \"longest\"):\n",
        "    chains = {}\n",
        "    chain_order = []\n",
        "    with pdb_path.open(\"r\") as handle:\n",
        "        for line in handle:\n",
        "            if not line.startswith(\"ATOM\"):\n",
        "                continue\n",
        "            if line[12:16].strip() != \"CA\":\n",
        "                continue\n",
        "            altloc = line[16]\n",
        "            if altloc not in (\" \", \"A\"):\n",
        "                continue\n",
        "            chain_id = line[21].strip() or \"_\"\n",
        "            try:\n",
        "                resseq = int(line[22:26].strip())\n",
        "            except ValueError:\n",
        "                continue\n",
        "            icode = line[26].strip() or \" \"\n",
        "            res_uid = (resseq, icode)\n",
        "            if chain_id not in chains:\n",
        "                chains[chain_id] = {\"coords\": [], \"seen\": set()}\n",
        "                chain_order.append(chain_id)\n",
        "            chain_data = chains[chain_id]\n",
        "            if res_uid in chain_data[\"seen\"]:\n",
        "                continue\n",
        "            try:\n",
        "                x = float(line[30:38])\n",
        "                y = float(line[38:46])\n",
        "                z = float(line[46:54])\n",
        "            except ValueError:\n",
        "                continue\n",
        "            chain_data[\"coords\"].append((x, y, z))\n",
        "            chain_data[\"seen\"].add(res_uid)\n",
        "\n",
        "    if not chains:\n",
        "        return None, {\"status\": \"no_ca\"}\n",
        "\n",
        "    chain_lengths = {cid: len(data[\"coords\"]) for cid, data in chains.items()}\n",
        "\n",
        "    if chain_strategy == \"single\":\n",
        "        if len(chains) != 1:\n",
        "            return None, {\"status\": \"multi_chain\", \"chain_lengths\": chain_lengths}\n",
        "        chain_id = chain_order[0]\n",
        "        coords = chains[chain_id][\"coords\"]\n",
        "    elif chain_strategy == \"longest\":\n",
        "        chain_id = max(chain_lengths, key=chain_lengths.get)\n",
        "        coords = chains[chain_id][\"coords\"]\n",
        "    elif chain_strategy == \"concat\":\n",
        "        coords = []\n",
        "        for cid in chain_order:\n",
        "            coords.extend(chains[cid][\"coords\"])\n",
        "        chain_id = \"+\".join(chain_order)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown chain_strategy: {chain_strategy}\")\n",
        "\n",
        "    if len(coords) == 0:\n",
        "        return None, {\"status\": \"no_ca\", \"chain_lengths\": chain_lengths}\n",
        "\n",
        "    coords_array = np.asarray(coords, dtype=np.float32)\n",
        "    meta = {\n",
        "        \"status\": \"ok\",\n",
        "        \"chain_id\": chain_id,\n",
        "        \"chain_lengths\": chain_lengths,\n",
        "        \"n_res\": coords_array.shape[0],\n",
        "        \"num_chains\": len(chain_lengths),\n",
        "    }\n",
        "    return coords_array, meta\n",
        "\n",
        "\n",
        "def pairwise_distances(coords: np.ndarray) -> np.ndarray:\n",
        "    coords = coords.astype(np.float32, copy=False)\n",
        "    sq = np.sum(coords ** 2, axis=1, keepdims=True)\n",
        "    dist2 = sq + sq.T - 2.0 * (coords @ coords.T)\n",
        "    np.maximum(dist2, 0.0, out=dist2)\n",
        "    np.sqrt(dist2, out=dist2)\n",
        "    return dist2\n",
        "\n",
        "\n",
        "def pad_or_trim(dist: np.ndarray, target_len: int, pad_value: float) -> tuple[np.ndarray, int]:\n",
        "    n = dist.shape[0]\n",
        "    used = min(n, target_len)\n",
        "    if n >= target_len:\n",
        "        return dist[:target_len, :target_len], used\n",
        "    out = np.full((target_len, target_len), pad_value, dtype=dist.dtype)\n",
        "    out[:n, :n] = dist\n",
        "    return out, used\n",
        "\n",
        "\n",
        "def dist_to_similarity(dist: np.ndarray, sigma: float) -> torch.Tensor:\n",
        "    dist_tensor = torch.from_numpy(dist)\n",
        "    sim = torch.exp(-(dist_tensor ** 2) / (2.0 * sigma ** 2))\n",
        "    mask = torch.isinf(dist_tensor) | torch.isnan(dist_tensor)\n",
        "    if mask.any():\n",
        "        sim.masked_fill_(mask, 0.0)\n",
        "    return sim.float()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "lengths = []\n",
        "chain_counts = []\n",
        "bad_records = []\n",
        "\n",
        "for pdb_path in pdb_files:\n",
        "    coords, meta = parse_ca_coords(pdb_path, chain_strategy=CHAIN_STRATEGY)\n",
        "    if coords is None:\n",
        "        bad_records.append((pdb_path.name, meta.get(\"status\", \"unknown\")))\n",
        "        continue\n",
        "    lengths.append(coords.shape[0])\n",
        "    chain_counts.append(meta[\"num_chains\"])\n",
        "\n",
        "print(f\"Valid coords: {len(lengths)} / {len(pdb_files)}\")\n",
        "if lengths:\n",
        "    print(\n",
        "        \"Min/median/max length:\",\n",
        "        int(min(lengths)),\n",
        "        int(np.median(lengths)),\n",
        "        int(max(lengths)),\n",
        "    )\n",
        "    print(f\"Most common chain counts: {Counter(chain_counts).most_common(5)}\")\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.hist(lengths, bins=50, color=\"steelblue\")\n",
        "    plt.title(\"Residue count distribution\")\n",
        "    plt.xlabel(\"N_res\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No valid entries found; adjust parsing settings.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you change `TARGET_L`, re-run the config cell and the precompute section below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "records = []\n",
        "status_counts = Counter()\n",
        "\n",
        "for idx, pdb_path in enumerate(pdb_files, 1):\n",
        "    coords, meta = parse_ca_coords(pdb_path, chain_strategy=CHAIN_STRATEGY)\n",
        "    if coords is None:\n",
        "        status = meta.get(\"status\", \"parse_fail\")\n",
        "        status_counts[status] += 1\n",
        "        records.append(\n",
        "            {\n",
        "                \"pdb_path\": str(pdb_path),\n",
        "                \"status\": status,\n",
        "                \"chain_id\": \"\",\n",
        "                \"n_res\": \"\",\n",
        "                \"n_used\": \"\",\n",
        "                \"sim_path\": \"\",\n",
        "            }\n",
        "        )\n",
        "        continue\n",
        "\n",
        "    n_res = coords.shape[0]\n",
        "    if n_res < MIN_RESIDUES:\n",
        "        status_counts[\"too_short\"] += 1\n",
        "        records.append(\n",
        "            {\n",
        "                \"pdb_path\": str(pdb_path),\n",
        "                \"status\": \"too_short\",\n",
        "                \"chain_id\": meta.get(\"chain_id\", \"\"),\n",
        "                \"n_res\": n_res,\n",
        "                \"n_used\": \"\",\n",
        "                \"sim_path\": \"\",\n",
        "            }\n",
        "        )\n",
        "        continue\n",
        "\n",
        "    dist = pairwise_distances(coords)\n",
        "    dist_padded, n_used = pad_or_trim(dist, TARGET_L, PAD_VALUE)\n",
        "\n",
        "    if np.isnan(dist_padded).any():\n",
        "        status_counts[\"nan\"] += 1\n",
        "        records.append(\n",
        "            {\n",
        "                \"pdb_path\": str(pdb_path),\n",
        "                \"status\": \"nan\",\n",
        "                \"chain_id\": meta.get(\"chain_id\", \"\"),\n",
        "                \"n_res\": n_res,\n",
        "                \"n_used\": \"\",\n",
        "                \"sim_path\": \"\",\n",
        "            }\n",
        "        )\n",
        "        continue\n",
        "\n",
        "    sim = dist_to_similarity(dist_padded, SIGMA)\n",
        "    sim_path = OUTPUT_DIR / f\"{pdb_path.stem}.pt\"\n",
        "    torch.save(sim, sim_path)\n",
        "\n",
        "    status_counts[\"ok\"] += 1\n",
        "    records.append(\n",
        "        {\n",
        "            \"pdb_path\": str(pdb_path),\n",
        "            \"status\": \"ok\",\n",
        "            \"chain_id\": meta.get(\"chain_id\", \"\"),\n",
        "            \"n_res\": n_res,\n",
        "            \"n_used\": n_used,\n",
        "            \"sim_path\": str(sim_path),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if idx % 500 == 0:\n",
        "        print(f\"Processed {idx}/{len(pdb_files)}\")\n",
        "\n",
        "with MANIFEST_PATH.open(\"w\", newline=\"\") as handle:\n",
        "    fieldnames = [\"pdb_path\", \"status\", \"chain_id\", \"n_res\", \"n_used\", \"sim_path\"]\n",
        "    writer = csv.DictWriter(handle, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(records)\n",
        "\n",
        "print(\"Precompute done. Status counts:\")\n",
        "print(status_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "with MANIFEST_PATH.open(\"r\", newline=\"\") as handle:\n",
        "    reader = csv.DictReader(handle)\n",
        "    manifest_records = list(reader)\n",
        "\n",
        "ok_records = [r for r in manifest_records if r[\"status\"] == \"ok\"]\n",
        "ok_lengths = [int(r[\"n_res\"]) for r in ok_records]\n",
        "\n",
        "print(f\"Kept {len(ok_records)} samples\")\n",
        "if ok_lengths:\n",
        "    print(\n",
        "        \"Kept min/median/max length:\",\n",
        "        int(min(ok_lengths)),\n",
        "        int(np.median(ok_lengths)),\n",
        "        int(max(ok_lengths)),\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.hist(ok_lengths, bins=50, color=\"seagreen\")\n",
        "    plt.title(\"Kept residue count distribution\")\n",
        "    plt.xlabel(\"N_res\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SimilarityDataset(Dataset):\n",
        "    def __init__(self, manifest_path: Path, return_mask: bool = False, add_channel: bool = False):\n",
        "        self.manifest_path = manifest_path\n",
        "        self.return_mask = return_mask\n",
        "        self.add_channel = add_channel\n",
        "\n",
        "        with manifest_path.open(\"r\", newline=\"\") as handle:\n",
        "            reader = csv.DictReader(handle)\n",
        "            self.records = [row for row in reader if row[\"status\"] == \"ok\"]\n",
        "\n",
        "        if not self.records:\n",
        "            raise ValueError(\"No valid samples in manifest.\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.records)\n",
        "\n",
        "    def _make_mask(self, n_used: int, shape: tuple[int, int] | tuple[int, int, int]) -> torch.Tensor:\n",
        "        mask = torch.zeros(shape, dtype=torch.bool)\n",
        "        if self.add_channel:\n",
        "            mask[:, :n_used, :n_used] = True\n",
        "        else:\n",
        "            mask[:n_used, :n_used] = True\n",
        "        return mask\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        rec = self.records[idx]\n",
        "        sim = torch.load(rec[\"sim_path\"])\n",
        "        if self.add_channel:\n",
        "            sim = sim.unsqueeze(0)\n",
        "        if self.return_mask:\n",
        "            n_used = int(rec[\"n_used\"])\n",
        "            mask = self._make_mask(n_used, sim.shape)\n",
        "            return sim, mask\n",
        "        return sim\n",
        "\n",
        "\n",
        "dataset = SimilarityDataset(MANIFEST_PATH, return_mask=True, add_channel=False)\n",
        "indices = list(range(len(dataset)))\n",
        "random.Random(RANDOM_SEED).shuffle(indices)\n",
        "split = int(0.8 * len(indices))\n",
        "train_idx, val_idx = indices[:split], indices[split:]\n",
        "\n",
        "train_ds = Subset(dataset, train_idx)\n",
        "val_ds = Subset(dataset, val_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=0)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "sim_batch, mask_batch = batch\n",
        "print(sim_batch.shape, mask_batch.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "sample_rec = dataset.records[0]\n",
        "sample_path = Path(sample_rec[\"pdb_path\"])\n",
        "\n",
        "coords, meta = parse_ca_coords(sample_path, chain_strategy=CHAIN_STRATEGY)\n",
        "dist = pairwise_distances(coords)\n",
        "dist_padded, n_used = pad_or_trim(dist, TARGET_L, PAD_VALUE)\n",
        "sim = dist_to_similarity(dist_padded, SIGMA).numpy()\n",
        "\n",
        "dist_vis = dist_padded.copy()\n",
        "dist_vis[~np.isfinite(dist_vis)] = np.nan\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].imshow(dist_vis, cmap=\"magma\")\n",
        "axes[0].set_title(\"Distogram\")\n",
        "\n",
        "axes[1].imshow(sim, cmap=\"viridis\", vmin=0.0, vmax=1.0)\n",
        "axes[1].set_title(\"Similarity\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}