{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0993f8",
   "metadata": {},
   "source": [
    "# Self-supervised gap-filling data loader\n",
    "\n",
    "This notebook loads the precomputed similarity tensors saved as `.pt` files,\n",
    "reads the manifest produced by the previous pipeline, and builds train/eval\n",
    "DataLoaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b3f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import random\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset, DataLoader, Subset\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\n",
    "        \"This notebook requires torch. Install it before running.\"\n",
    "    ) from exc\n",
    "\n",
    "DATA_ROOT = Path(\"data\")\n",
    "PRECOMPUTED_DIR = None  # set Path(\"data/precomputed_L100\") if needed\n",
    "RANDOM_SEED = 42\n",
    "TRAIN_FRACTION = 0.8\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0\n",
    "RETURN_MASK = False\n",
    "ADD_CHANNEL = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e7af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using precomputed dir: data/precomputed_L100\n",
      "Manifest: data/precomputed_L100/manifest.csv\n",
      "Total records: 6631\n",
      "OK records: 6631\n"
     ]
    }
   ],
   "source": [
    "def _extract_len(path: Path) -> int:\n",
    "    prefix = \"precomputed_L\"\n",
    "    if not path.name.startswith(prefix):\n",
    "        return -1\n",
    "    try:\n",
    "        return int(path.name[len(prefix) :])\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def _find_precomputed(root: Path) -> Path:\n",
    "    candidates = sorted(root.glob(\"precomputed_L*\"), key=_extract_len)\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"No precomputed_L* directory found in data/.\")\n",
    "    return candidates[-1]\n",
    "\n",
    "\n",
    "if PRECOMPUTED_DIR is None:\n",
    "    PRECOMPUTED_DIR = _find_precomputed(DATA_ROOT)\n",
    "\n",
    "MANIFEST_PATH = PRECOMPUTED_DIR / \"manifest.csv\"\n",
    "if not MANIFEST_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing manifest: {MANIFEST_PATH}\")\n",
    "\n",
    "print(f\"Using precomputed dir: {PRECOMPUTED_DIR}\")\n",
    "print(f\"Manifest: {MANIFEST_PATH}\")\n",
    "\n",
    "with MANIFEST_PATH.open(\"r\", newline=\"\") as handle:\n",
    "    reader = csv.DictReader(handle)\n",
    "    manifest_records = list(reader)\n",
    "\n",
    "ok_records = [r for r in manifest_records if r[\"status\"] == \"ok\"]\n",
    "print(f\"Total records: {len(manifest_records)}\")\n",
    "print(f\"OK records: {len(ok_records)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce355ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 5304\n",
      "Eval samples: 1327\n",
      "Batch shape: torch.Size([64, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "class SimilarityDataset(Dataset):\n",
    "    def __init__(self, records, return_mask: bool = False, add_channel: bool = False):\n",
    "        self.records = list(records)\n",
    "        self.return_mask = return_mask\n",
    "        self.add_channel = add_channel\n",
    "\n",
    "        if not self.records:\n",
    "            raise ValueError(\"No records provided to the dataset.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        rec = self.records[idx]\n",
    "        sim = torch.load(rec[\"sim_path\"])\n",
    "        if self.add_channel:\n",
    "            sim = sim.unsqueeze(0)\n",
    "\n",
    "        if self.return_mask:\n",
    "            n_used = int(rec[\"n_used\"]) if rec[\"n_used\"] else sim.shape[-1]\n",
    "            mask = torch.zeros_like(sim, dtype=torch.bool)\n",
    "            mask[..., :n_used, :n_used] = True\n",
    "            return sim, mask\n",
    "        return sim\n",
    "\n",
    "\n",
    "dataset = SimilarityDataset(ok_records, return_mask=RETURN_MASK, add_channel=ADD_CHANNEL)\n",
    "indices = list(range(len(dataset)))\n",
    "random.Random(RANDOM_SEED).shuffle(indices)\n",
    "\n",
    "split = int(TRAIN_FRACTION * len(indices))\n",
    "train_idx, eval_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "eval_ds = Subset(dataset, eval_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Eval samples: {len(eval_ds)}\")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "if RETURN_MASK:\n",
    "    sim_batch, mask_batch = batch\n",
    "    print(\"Batch shapes:\", sim_batch.shape, mask_batch.shape)\n",
    "else:\n",
    "    print(\"Batch shape:\", batch.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
